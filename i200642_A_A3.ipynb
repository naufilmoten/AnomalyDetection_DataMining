{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a353de",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'ContrastiveLoss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8312\\3252846543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m# Loss functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mcriterion_gan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mcontrastive_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContrastiveLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m# Data preparation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'ContrastiveLoss'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the Transformer Autoencoder model\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        # Decoder layers\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=input_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded, encoded)  # Using encoded output as memory\n",
    "        return self.linear(decoded)\n",
    "\n",
    "# Define the Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, autoencoder_model):\n",
    "        super(Generator, self).__init__()\n",
    "        # Use the TransformerAutoencoder model as the generator\n",
    "        self.autoencoder = autoencoder_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generator generates new data instances\n",
    "        return self.autoencoder(x)\n",
    "\n",
    "# Define the Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('creditcard.csv').drop_duplicates().dropna()\n",
    "data_normalized = (df - df.mean()) / df.std()\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 31\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "num_epochs = 5\n",
    "batch_size = 8\n",
    "margin = 1.0\n",
    "p = 0.1\n",
    "\n",
    "# Initialize models and optimizers\n",
    "model = TransformerAutoencoder(input_dim, hidden_dim, num_layers, num_heads)\n",
    "generator = Generator(model)\n",
    "discriminator = Discriminator(input_dim, hidden_dim)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "ae_optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss functions\n",
    "criterion_gan = nn.BCELoss()\n",
    "contrastive_loss = nn.ContrastiveLoss(margin=margin)\n",
    "\n",
    "# Data preparation\n",
    "data_normalized_tensor = torch.tensor(data_normalized.values, dtype=torch.float32)\n",
    "data_loader = DataLoader(data_normalized_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data in data_loader:\n",
    "        batch_size, sequence_length = data.shape\n",
    "        mask = torch.rand(batch_size, sequence_length) < p\n",
    "        positive_data = data.clone()\n",
    "        positive_data[mask] = 0\n",
    "        negative_data = torch.roll(data, shifts=1, dims=0)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        real_output = discriminator(data)\n",
    "        real_loss = criterion_gan(real_output, torch.ones_like(real_output))\n",
    "        fake_data = generator(data)\n",
    "        fake_output = discriminator(fake_data.detach())\n",
    "        fake_loss = criterion_gan(fake_output, torch.zeros_like(fake_output))\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        trick_output = discriminator(fake_data)\n",
    "        g_loss = criterion_gan(trick_output, torch.ones_like(trick_output))\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Train Autoencoder with Contrastive Learning\n",
    "        ae_optimizer.zero_grad()\n",
    "        original_recon = model(data)\n",
    "        positive_recon = model(positive_data)\n",
    "        negative_recon = model(negative_data)\n",
    "        pos_loss = contrastive_loss(original_recon, positive_recon, torch.ones(data.size(0)))\n",
    "        neg_loss = contrastive_loss(original_recon, negative_recon, torch.zeros(data.size(0)))\n",
    "        ae_loss = pos_loss + neg_loss\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, AE Loss: {ae_loss.item():.4f}')\n",
    "\n",
    "# Metric calculation function\n",
    "def calculate_metrics(predicted, actual, threshold=0.1):\n",
    "    mse = torch.mean((predicted - actual) ** 2).item()\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(predicted - actual)).item()\n",
    "    correct_predictions = torch.abs(predicted - actual) <= threshold\n",
    "    accuracy = torch.mean(correct_predictions.float()).item()\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse.item(), \"MAE\": mae, \"Accuracy\": accuracy}\n",
    "\n",
    "# Evaluate metrics\n",
    "for epoch in range(num_epochs):\n",
    "    predicted_all = []\n",
    "    actual_all = []\n",
    "    for data in data_loader:\n",
    "        with torch.no_grad():\n",
    "            predicted = model(data)\n",
    "            actual = data\n",
    "            predicted_all.append(predicted)\n",
    "            actual_all.append(actual)\n",
    "\n",
    "    predicted_tensor = torch.cat(predicted_all, 0)\n",
    "    actual_tensor = torch.cat(actual_all, 0)\n",
    "\n",
    "    metrics = calculate_metrics(predicted_tensor, actual_tensor)\n",
    "    print(f\"Epoch {epoch+1} Metrics:\", metrics)\n",
    "\n",
    "# Anomaly detection function\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    anomalies = []\n",
    "    for data in data_loader:\n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(data)\n",
    "            errors = torch.mean((data - reconstructed) ** 2, dim=1)\n",
    "            anomalies.extend([(i, error.item()) for i, error in enumerate(errors) if error.item() > threshold])\n",
    "    return anomalies\n",
    "\n",
    "# Detect anomalies\n",
    "anomaly_threshold = 0.1\n",
    "anomalies = detect_anomalies(data_loader, model, anomaly_threshold)\n",
    "\n",
    "# Print anomalies\n",
    "print(\"Anomalies:\")\n",
    "for index, error in anomalies:\n",
    "    print(f\"Index: {index}, Reconstruction Error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb619e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
